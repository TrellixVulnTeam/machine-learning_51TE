{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective: Build character corpus using a Recurrent Neural Network\n",
    "\n",
    "Reading moby_dick.txt as data source. Taken from mody_dick-ORIG.txt which was downloaded from Project Gutenberg\n",
    "\n",
    "Refer to this:\n",
    "https://www.tensorflow.org/tutorials/text/text_generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "﻿CHAPTER 1. Loomings.\n",
      "\n",
      "Call me Ishmael. Some years ago—never mind how long precisely—having\n",
      "little or no money in my purse, and nothing particular to interest me\n",
      "on shore, I thought I would sail about a little and see the watery part\n",
      "of the world. It\n"
     ]
    }
   ],
   "source": [
    "# Read text\n",
    "text = open('moby_dick.txt', 'r', encoding=\"utf8\").read()\n",
    "\n",
    "# Print first 250 characters\n",
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '!', '$', '&', '(', ')', '*', ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '£', 'â', 'æ', 'è', 'é', 'œ', '—', '‘', '’', '“', '”', '\\ufeff']\n"
     ]
    }
   ],
   "source": [
    "# Find all unique characters in the text\n",
    "vocab = sorted(set(text))\n",
    "\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1191787 total characters\n",
      "91 unique characters\n"
     ]
    }
   ],
   "source": [
    "print('{} total characters'.format(len(text)))\n",
    "print('{} unique characters'.format(len(vocab)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize the text\n",
    "\n",
    "Create a mapping from unique characters to a numerical representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "char_to_idx = { ch:i for i, ch in enumerate(vocab) }\n",
    "idx_to_char = np.array(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[90 26 31 24 39 43 28 41  1 12 10  1 35 67 67 65 61 66 59 71 10  0  0 26\n",
      " 53 64 64  1 65 57  1 32 71 60 65 53 57 64 10  1 42 67 65 57  1 77 57 53\n",
      " 70 71  1 53 59 67 85 66 57 74 57 70  1 65 61 66 56  1 60 67 75  1 64 67\n",
      " 66 59  1 68 70 57 55 61 71 57 64 77 85 60 53 74 61 66 59  0 64 61 72 72\n",
      " 64 57  1 67 70  1 66 67  1 65 67 66 57 77  1 61 66  1 65 77  1 68 73 70\n",
      " 71 57  8  1 53 66 56  1 66 67 72 60 61 66 59  1 68 53 70 72 61 55 73 64\n",
      " 53 70  1 72 67  1 61 66 72 57 70 57 71 72  1 65 57  0 67 66  1 71 60 67\n",
      " 70 57  8  1 32  1 72 60 67 73 59 60 72  1 32  1 75 67 73 64 56  1 71 53\n",
      " 61 64  1 53 54 67 73 72  1 53  1 64 61 72 72 64 57  1 53 66 56  1 71 57\n",
      " 57  1 72 60 57  1 75 53 72 57 70 77  1 68 53 70 72  0 67 58  1 72 60 57\n",
      "  1 75 67 70 64 56 10  1 32 72]\n"
     ]
    }
   ],
   "source": [
    "# Map all characters in text to ints\n",
    "text_as_int = np.array([ char_to_idx[c] for c in text ])\n",
    "\n",
    "print(text_as_int[:250])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "﻿\n",
      "C\n",
      "H\n",
      "A\n",
      "P\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Max length sentence for a single input of chars\n",
    "seq_length = 100\n",
    "examples_per_epoch = len(text) // (seq_length + 1)\n",
    "\n",
    "# Create training examples\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "\n",
    "for i in char_dataset.take(5):\n",
    "    print(idx_to_char[i.numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'\\ufeffCHAPTER 1. Loomings.\\n\\nCall me Ishmael. Some years ago—never mind how long precisely—having\\nlittle or'\n",
      "' no money in my purse, and nothing particular to interest me\\non shore, I thought I would sail about a'\n",
      "' little and see the watery part\\nof the world. It is a way I have of driving off the spleen and\\nregula'\n",
      "'ting the circulation. Whenever I find myself growing grim about\\nthe mouth; whenever it is a damp, dri'\n",
      "'zzly November in my soul; whenever\\nI find myself involuntarily pausing before coffin warehouses, and\\n'\n"
     ]
    }
   ],
   "source": [
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "for item in sequences.take(5):\n",
    "    print(repr(''.join(idx_to_char[item.numpy()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:  '\\ufeffCHAPTER 1. Loomings.\\n\\nCall me Ishmael. Some years ago—never mind how long precisely—having\\nlittle o'\n",
      "Target:  'CHAPTER 1. Loomings.\\n\\nCall me Ishmael. Some years ago—never mind how long precisely—having\\nlittle or'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for input_example, target_example in dataset.take(1):\n",
    "    print('Input: ', repr(''.join(idx_to_char[input_example.numpy()])))\n",
    "    print('Target: ', repr(''.join(idx_to_char[target_example.numpy()])))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step {:4d}: 0\n",
      "\tinput: 90 ('\\ufeff')\n",
      "\texpected output: 26 ('C')\n",
      "Step {:4d}: 1\n",
      "\tinput: 26 ('C')\n",
      "\texpected output: 31 ('H')\n",
      "Step {:4d}: 2\n",
      "\tinput: 31 ('H')\n",
      "\texpected output: 24 ('A')\n",
      "Step {:4d}: 3\n",
      "\tinput: 24 ('A')\n",
      "\texpected output: 39 ('P')\n",
      "Step {:4d}: 4\n",
      "\tinput: 39 ('P')\n",
      "\texpected output: 43 ('T')\n"
     ]
    }
   ],
   "source": [
    "for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\n",
    "    print(\"Step {:4d}:\", format(i))\n",
    "    print(\"\\tinput: {} ({:s})\".format(input_idx, repr(idx_to_char[input_idx])))\n",
    "    print(\"\\texpected output: {} ({:s})\".format(target_idx, repr(idx_to_char[target_idx])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((64, 100), (64, 100)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create training batches\n",
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 256       # embedding dimension\n",
    "\n",
    "RNN_UNITS = 1024          # Number of RNN units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, GRU, Dense\n",
    "\n",
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    model = Sequential([\n",
    "        Embedding(vocab_size, embedding_dim, batch_input_shape=[batch_size, None]),\n",
    "        GRU(rnn_units, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'),\n",
    "        #LSTM\n",
    "        Dense(vocab_size)\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build it\n",
    "model = build_model(\n",
    "    vocab_size = len(vocab),\n",
    "    embedding_dim = EMBEDDING_DIM,\n",
    "    rnn_units = RNN_UNITS,\n",
    "    batch_size = BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 100, 91) \t(batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(example_batch_predictions.shape, \"\\t(batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (64, None, 256)           23296     \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    (64, None, 1024)          3938304   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (64, None, 91)            93275     \n",
      "=================================================================\n",
      "Total params: 4,054,875\n",
      "Trainable params: 4,054,875\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([57, 29, 54, 51, 32, 46, 57, 79, 64,  3, 21, 79, 68, 76, 81, 60, 32,\n",
       "       47, 81, 49, 10, 63, 65, 35,  7, 36, 83, 18, 18, 31, 16, 75,  6, 83,\n",
       "       21,  8, 78, 41, 22, 31, 45, 83, 46, 11, 65, 59, 27, 39, 59, 11, 64,\n",
       "       61, 15, 61, 60, 66, 11, 87, 68, 55, 81, 15, 71, 81, 22, 37, 17, 11,\n",
       "       49, 60, 76, 76, 69, 30, 86,  5, 23, 23, 49, 90, 24, 62, 47, 15, 88,\n",
       "       21, 68, 66, 38, 62, 69, 81, 10, 47, 56, 18, 80,  9, 78, 34],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Try first example in branch\n",
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()\n",
    "sampled_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: \n",
      " 'or I’ll be combing ye!”\\n\\n“Come on, Queequeg,” said I, “all right. There’s Mrs. Hussey.”\\n\\nAnd so it t'\n",
      "\n",
      "Next Char Predictions: \n",
      " 'eFb]IWe£l$:£pxæhIXæZ.kmL*Mé77H5w)é:,zR;HVéW0mgDPg0li4ihn0’pcæ4sæ;N60ZhxxqG‘(??Z\\ufeffAjX4“:pnOjqæ.Xd7â-zK'\n"
     ]
    }
   ],
   "source": [
    "print(\"Input: \\n\", repr(\"\".join(idx_to_char[input_example_batch[0]])))\n",
    "print()\n",
    "print(\"Next Char Predictions: \\n\", repr(\"\".join(idx_to_char[sampled_indices])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape:  (64, 100, 91)  # (batch_size, sequence_length, vocab_size)\n",
      "scalar_loss:       4.5107417\n"
     ]
    }
   ],
   "source": [
    "# Set up loss function with logits\n",
    "from tensorflow.keras.losses import sparse_categorical_crossentropy\n",
    "\n",
    "def loss(labels, logits):\n",
    "    return sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "\n",
    "example_batch_loss = loss(target_example_batch, example_batch_predictions)\n",
    "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
    "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Configure training procedure\n",
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

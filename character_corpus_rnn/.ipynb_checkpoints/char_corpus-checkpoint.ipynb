{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective: Build character corpus using a Recurrent Neural Network\n",
    "\n",
    "Reading moby_dick.txt as data source. Taken from mody_dick-ORIG.txt which was downloaded from Project Gutenberg\n",
    "\n",
    "Refer to this:\n",
    "https://www.tensorflow.org/tutorials/text/text_generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "﻿CHAPTER 1. Loomings.\n",
      "\n",
      "Call me Ishmael. Some years ago—never mind how long precisely—having\n",
      "little or no money in my purse, and nothing particular to interest me\n",
      "on shore, I thought I would sail about a little and see the watery part\n",
      "of the world. It\n"
     ]
    }
   ],
   "source": [
    "# Read text\n",
    "text = open('moby_dick.txt', 'r', encoding=\"utf8\").read()\n",
    "\n",
    "# Print first 250 characters\n",
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '!', '$', '&', '(', ')', '*', ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '£', 'â', 'æ', 'è', 'é', 'œ', '—', '‘', '’', '“', '”', '\\ufeff']\n"
     ]
    }
   ],
   "source": [
    "# Find all unique characters in the text\n",
    "vocab = sorted(set(text))\n",
    "\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1191787 total characters\n",
      "91 unique characters\n"
     ]
    }
   ],
   "source": [
    "print('{} total characters'.format(len(text)))\n",
    "print('{} unique characters'.format(len(vocab)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize the text\n",
    "\n",
    "Create a mapping from unique characters to a numerical representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "char_to_idx = { ch:i for i, ch in enumerate(vocab) }\n",
    "idx_to_char = np.array(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[90 26 31 24 39 43 28 41  1 12 10  1 35 67 67 65 61 66 59 71 10  0  0 26\n",
      " 53 64 64  1 65 57  1 32 71 60 65 53 57 64 10  1 42 67 65 57  1 77 57 53\n",
      " 70 71  1 53 59 67 85 66 57 74 57 70  1 65 61 66 56  1 60 67 75  1 64 67\n",
      " 66 59  1 68 70 57 55 61 71 57 64 77 85 60 53 74 61 66 59  0 64 61 72 72\n",
      " 64 57  1 67 70  1 66 67  1 65 67 66 57 77  1 61 66  1 65 77  1 68 73 70\n",
      " 71 57  8  1 53 66 56  1 66 67 72 60 61 66 59  1 68 53 70 72 61 55 73 64\n",
      " 53 70  1 72 67  1 61 66 72 57 70 57 71 72  1 65 57  0 67 66  1 71 60 67\n",
      " 70 57  8  1 32  1 72 60 67 73 59 60 72  1 32  1 75 67 73 64 56  1 71 53\n",
      " 61 64  1 53 54 67 73 72  1 53  1 64 61 72 72 64 57  1 53 66 56  1 71 57\n",
      " 57  1 72 60 57  1 75 53 72 57 70 77  1 68 53 70 72  0 67 58  1 72 60 57\n",
      "  1 75 67 70 64 56 10  1 32 72]\n"
     ]
    }
   ],
   "source": [
    "# Map all characters in text to ints\n",
    "text_as_int = np.array([ char_to_idx[c] for c in text ])\n",
    "\n",
    "print(text_as_int[:250])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "﻿\n",
      "C\n",
      "H\n",
      "A\n",
      "P\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Max length sentence for a single input of chars\n",
    "seq_length = 100\n",
    "examples_per_epoch = len(text) // (seq_length + 1)\n",
    "\n",
    "# Create training examples\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "\n",
    "for i in char_dataset.take(5):\n",
    "    print(idx_to_char[i.numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'\\ufeffCHAPTER 1. Loomings.\\n\\nCall me Ishmael. Some years ago—never mind how long precisely—having\\nlittle or'\n",
      "' no money in my purse, and nothing particular to interest me\\non shore, I thought I would sail about a'\n",
      "' little and see the watery part\\nof the world. It is a way I have of driving off the spleen and\\nregula'\n",
      "'ting the circulation. Whenever I find myself growing grim about\\nthe mouth; whenever it is a damp, dri'\n",
      "'zzly November in my soul; whenever\\nI find myself involuntarily pausing before coffin warehouses, and\\n'\n"
     ]
    }
   ],
   "source": [
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "for item in sequences.take(5):\n",
    "    print(repr(''.join(idx_to_char[item.numpy()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:  '\\ufeffCHAPTER 1. Loomings.\\n\\nCall me Ishmael. Some years ago—never mind how long precisely—having\\nlittle o'\n",
      "Target:  'CHAPTER 1. Loomings.\\n\\nCall me Ishmael. Some years ago—never mind how long precisely—having\\nlittle or'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for input_example, target_example in dataset.take(1):\n",
    "    print('Input: ', repr(''.join(idx_to_char[input_example.numpy()])))\n",
    "    print('Target: ', repr(''.join(idx_to_char[target_example.numpy()])))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step {:4d}: 0\n",
      "\tinput: 90 ('\\ufeff')\n",
      "\texpected output: 26 ('C')\n",
      "Step {:4d}: 1\n",
      "\tinput: 26 ('C')\n",
      "\texpected output: 31 ('H')\n",
      "Step {:4d}: 2\n",
      "\tinput: 31 ('H')\n",
      "\texpected output: 24 ('A')\n",
      "Step {:4d}: 3\n",
      "\tinput: 24 ('A')\n",
      "\texpected output: 39 ('P')\n",
      "Step {:4d}: 4\n",
      "\tinput: 39 ('P')\n",
      "\texpected output: 43 ('T')\n"
     ]
    }
   ],
   "source": [
    "for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\n",
    "    print(\"Step {:4d}:\", format(i))\n",
    "    print(\"\\tinput: {} ({:s})\".format(input_idx, repr(idx_to_char[input_idx])))\n",
    "    print(\"\\texpected output: {} ({:s})\".format(target_idx, repr(idx_to_char[target_idx])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((64, 100), (64, 100)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create training batches\n",
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 256       # embedding dimension\n",
    "\n",
    "RNN_UNITS = 1024          # Number of RNN units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, GRU, Dense\n",
    "\n",
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    model = Sequential([\n",
    "        Embedding(vocab_size, embedding_dim, batch_input_shape=[batch_size, None]),\n",
    "        GRU(rnn_units, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'),\n",
    "        #LSTM\n",
    "        Dense(vocab_size)\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build it\n",
    "model = build_model(\n",
    "    vocab_size = len(vocab),\n",
    "    embedding_dim = EMBEDDING_DIM,\n",
    "    rnn_units = RNN_UNITS,\n",
    "    batch_size = BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 100, 91) \t(batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(example_batch_predictions.shape, \"\\t(batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (64, None, 256)           23296     \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    (64, None, 1024)          3938304   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (64, None, 91)            93275     \n",
      "=================================================================\n",
      "Total params: 4,054,875\n",
      "Trainable params: 4,054,875\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([24, 75, 66, 76, 44,  5, 37, 11, 82, 27, 86, 13, 46, 30, 17, 46, 43,\n",
       "       14, 87, 10, 61, 82, 64, 62, 66, 16, 54, 29, 50, 89, 63, 29, 13, 86,\n",
       "        2, 59, 67, 77, 73, 53, 22,  2, 78, 89, 43, 21,  8, 70, 78, 38, 45,\n",
       "       68, 77, 83,  5, 42, 70, 32, 63, 10, 25, 14,  6,  5, 22, 73, 13, 31,\n",
       "       19,  2, 84,  4, 45, 30, 55, 56, 34, 81, 10, 34, 38, 76, 24, 60, 53,\n",
       "       31, 61, 86, 87,  2, 11, 86, 46, 34, 60, 31, 43, 23, 12,  3],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Try first example in branch\n",
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()\n",
    "sampled_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: \n",
      " '. You would almost think a\\ngreat gun had been discharged; and if you noticed the light wreath of\\nvap'\n",
      "\n",
      "Next Char Predictions: \n",
      " 'AwnxU(N0èD‘2WG6WT3’.ièljn5bF[”kF2‘!goyua;!z”T:,rzOVpyé(SrIk.B3)(;u2H8!œ&VGcdKæ.KOxAhaHi‘’!0‘WKhHT?1$'\n"
     ]
    }
   ],
   "source": [
    "print(\"Input: \\n\", repr(\"\".join(idx_to_char[input_example_batch[0]])))\n",
    "print()\n",
    "print(\"Next Char Predictions: \\n\", repr(\"\".join(idx_to_char[sampled_indices])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape:  (64, 100, 91)  # (batch_size, sequence_length, vocab_size)\n",
      "scalar_loss:       4.510721\n"
     ]
    }
   ],
   "source": [
    "# Set up loss function with logits\n",
    "from tensorflow.keras.losses import sparse_categorical_crossentropy\n",
    "\n",
    "def loss(labels, logits):\n",
    "    return sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "\n",
    "example_batch_loss = loss(target_example_batch, example_batch_predictions)\n",
    "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
    "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Configure training procedure\n",
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import os\n",
    "\n",
    "# Configure checkpoints\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(filepath = checkpoint_prefix, save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "184/184 [==============================] - 598s 3s/step - loss: 2.6298\n",
      "Epoch 2/10\n",
      "184/184 [==============================] - 606s 3s/step - loss: 2.0099\n",
      "Epoch 3/10\n",
      "184/184 [==============================] - 601s 3s/step - loss: 1.7683\n",
      "Epoch 4/10\n",
      "184/184 [==============================] - 599s 3s/step - loss: 1.6209\n",
      "Epoch 5/10\n",
      "184/184 [==============================] - 603s 3s/step - loss: 1.5238\n",
      "Epoch 6/10\n",
      "184/184 [==============================] - 595s 3s/step - loss: 1.4576\n",
      "Epoch 7/10\n",
      "184/184 [==============================] - 600s 3s/step - loss: 1.4062\n",
      "Epoch 8/10\n",
      "184/184 [==============================] - 603s 3s/step - loss: 1.3624\n",
      "Epoch 9/10\n",
      "184/184 [==============================] - 597s 3s/step - loss: 1.3239\n",
      "Epoch 10/10\n",
      "184/184 [==============================] - 600s 3s/step - loss: 1.2885\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "\n",
    "history = model.fit(dataset, epochs=EPOCHS, callbacks = [checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./training_checkpoints\\\\ckpt_10'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.latest_checkpoint(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(len(vocab), EMBEDDING_DIM, RNN_UNITS, batch_size=1)\n",
    "\n",
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "\n",
    "model.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (1, None, 256)            23296     \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (1, None, 1024)           3938304   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (1, None, 91)             93275     \n",
      "=================================================================\n",
      "Total params: 4,054,875\n",
      "Trainable params: 4,054,875\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate text using the learned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate text\n",
    "def generate_text(model, start_string, num_generate = 1000):\n",
    "    \n",
    "    # Convert start string into numbers\n",
    "    input_eval = [ char_to_idx[s] for s in start_string ]\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "    \n",
    "    text_generated = []\n",
    "    temperature = 1.0    # low to predictable text vs high for surprising text\n",
    "    \n",
    "    model.reset_states()\n",
    "    for i in range(num_generate):\n",
    "        predictions = model(input_eval)\n",
    "        \n",
    "        # remove batch dimension\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "        \n",
    "        # use a categorical distribution to predict the character returned by the model\n",
    "        predictions = predictions / temperature\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy()\n",
    "        \n",
    "        # pass the predicted character as the next input for the model along with the prev hidden state\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "        \n",
    "        text_generated.append(idx_to_char[predicted_id])\n",
    "        \n",
    "    return (start_string + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greatht Captain\n",
      "Leviathan shallowed to man’s to spouting on a horizontal instant away.\n",
      "\n",
      "The rush whole an enchanter’s bodies for whalemen of those forft. And yet he fell, Bildad, after\n",
      "and solicient latituse: and mays, D’yir thing—”\n",
      "\n",
      "“Certainmisly blows! Queequeg!\n",
      "Wide Starb While the mawers of ark with a lexced into\n",
      "whose whalement is ofteneath the food of the air, but to give me to his spout is far terms, circumstance, that\n",
      "sect of the captain thanqued and will, and wides archilete; as the same\n",
      "instant circling hither, like a flit out of experiment you must go, for up his\n",
      "blood. Sirribly impitably to hunt that time in its uncentrable! Do ye, spotts, had regenerated sinking spoutings towards the\n",
      "stifles without the ship; and, how he sideways to flew through me, making\n",
      "is still chanced thy\n",
      "simil us. Till all four years or they were the last repose of latitude, that to it!\n",
      "\n",
      "THo is fe they caught by the time of old Rook of Jonah, their who saved a\n",
      "very harpooneer you all more venticues! Truck, like the front eform, he had been\n",
      "divided among those who had been three yeardened clee erect still to the\n",
      "short, sir; I could like that, I must have full\n",
      "frighted boat’s bolted bound on as than Ramper and did just as they were, and I might have\n",
      "tight and longitude with its, that indeed spear of applumentions of one\n",
      "hundred affidinvirial thing of the green or the fand of die in his voyage, stranged by\n",
      "anduled wrapped; however front out of an eye on the vary of the sea.\n",
      "The all-sails all catch his precious twisted\n",
      "twing plunged for all down away an Eruople upon him as couldness, whom over them whether thy whale, in obliqueny showly out a\n",
      "beach ting, will, on doing delivery, all the ground eformous extremes. I have be\n",
      "a gloom or two have sided in the whale; they did I have shortly! teety, to swime\n",
      "they the great Batheric villains, they stooden by such the time seemed their hatch went out under the\n",
      "trages; of friendlit rustures aloft to the glad and influctedent; but by that?”\n",
      "\n",
      "\n",
      "CHAPTER 63. The Captain and away\n",
      "the young acceuting itemstricts\n",
      "modified; Prutanity rolling as grew spork. The slipping we\n",
      "stume—nothing but one, ling out from any fine years ago. Chere have been\n",
      "the chase, twisting a\n",
      "constimation of the evils of the Antir-Whale’s head at forty.\n",
      "\n",
      "“Who’s sometimes Nammered as Ahab took its conveysane.\n",
      "Passus and lines and swiftly goodness together. All about, invaluebast of these\n",
      "ioromotion (also had gone to the last a\n",
      "marigon. Itting the presence this\n",
      "captain have feeted there; and still founded, then, why drinking over one, he\n",
      "caulked about sully and hammock!”\n",
      "\n",
      "“There is the rushed hours on the sun start hanging the ground with his boat.\n",
      "\n",
      "“There all the shape, day into their structure; while\n",
      "erected forty young oars, to enterrip. Rain, I suppose, so\n",
      "that he does almost starting out that the prow—the sage of teeth, nay, and the\n",
      "species, then will trysan temporary.\n",
      "‘Wounding at the last climeration had bedind to the Cevelong hends of moluminy rather? Up!\n",
      "\n",
      "\n",
      "Aye, aye, and grow her\n",
      "yourself; is the Greenland Finishes; for what you field take\n",
      "this, he sawstances, delibed (its werried; and where you, the\n",
      "three savage’s eyes lie upon you clear sorry in the fourth one.\n",
      "\n",
      "Now:\n",
      "georial dogrinf, you cannot how himself along what\n",
      "are distingually long. As it who invour boats still\n",
      "sailing on us consiberating limitment.\n",
      "The twins of the White Whale\n",
      "freely out of him, Captiziong harpooneer is but unscrowled, though secondal, in the attemption he\n",
      "descried the curve and Captain\n",
      "in view to anyth general minutes no means of the or gold, he would I\n",
      "would not come to the stern; and for a people of previous, and separately\n",
      "the bid, and flung hammer\n",
      "there, as for your nakedness. It was crowding on light. The hands to after his oil\n",
      "blind head, or sand,\n",
      "\n",
      "but you will do thee.”\n",
      "\n",
      "As lengthelf, in that further white bitto\n",
      "seothed head and breadons as an one with the ship. Though the\n",
      "two them at that same time, and called to, and Frank the first year (lebbons,” said the sea.\n",
      "\n",
      "Now, tat of his enough half-erectest in that girltion of command, as And so that the\n",
      "crew, most ticks to give it on the other gurining—these, and tend to look as anawy in the\n",
      "whale-end confolediness of Aham withdrawly account in this\n",
      "hold the agies, no more of eightness, till you be antiquity you please.\n",
      "\n",
      "The side-vinging mouth is coming downs! Queequeg, have s as to\n",
      "be the equatorial skin thale, you know that he had not rushly interveniest on,\n",
      "the Pigging poises that a good girl!—step my took or the antioich Coleable! There, jewells, he\n",
      "malest one lamp-island gioble, never at\n",
      "last seemed that I would have\n",
      "no han thut closely savage whether it is citateor I must be\n",
      "their funning up to such miles and five a distrust of him, his pilot-phantom shows\n",
      "towing all this, when exactly, doubtless now entirely high-town to you, D.\n",
      "ENoI aftered Starbuck turning at a time. As these harpoon from the boat!\n",
      "the sea, as the horn-mouthed slip only waved. They look at all; that the\n",
      "Ruffy, and Marbean?”\n",
      "\n",
      "“Time.\n",
      "\n",
      "At last that that after a pedlor mah! From here, the strope dog is gringed. They glided to\n",
      "hammock. This was ready the mouthful right of their supports being three Determing into a coat.\n",
      "\n",
      "With the e jey minds of the water; and only all\n",
      "issed mocestines and among the common man; however the watch,\n",
      "and there’s no external decks once\n",
      "bear power. Creadily, into\n",
      "them.\n",
      "\n",
      "“All I keep your points. I’ll now marbled by lean now, and then\n",
      "slipped at the holeas of the deck eventanning for him. For us all. But gave\n",
      "your copper.\n",
      "\n",
      "No ise toursen; upon which the hards of his tones all\n",
      "deteration of him, so interceddenly, that the\n",
      "school of his head and squids of Truept, slips him he can—“deem it be the\n",
      "apprehension—a terror in the treater of the ship’s deck, ahab’s harpoons as\n",
      "shoulders, somewhere must never mund there.’\n",
      "\n",
      "“‘No, to _thir_, till you do, rot vely adventurous communished\n",
      "sights? Well, boyn!—Jake thee, ye had be no couraged from his lines, I will not\n",
      "coure do anything is in the whale-chipon,\n",
      "that word come to under the Anternal;\n",
      "unascumbering that never came you to himself: I hovering us, thank of\n",
      "the entire centre, and every day more for fourly ship—that sweet\n",
      "sunses in ull his head. The surface, as float one, each roubleon;\n",
      "imputed by the profining concerning the rose fact\n",
      "solt uprilsed on both Circumstance there, is thysells were stood of taling and tot; and it was\n",
      "now reively acquainted as the chbsholl can.\n",
      "Though—somehow, “Alive! His old age, it was only twe during whale pull forty\n",
      "that he eregantly, and, af screweded their passage one morning like an\n",
      "abasts before them, on The full glow replies of indirst\n",
      "burmak with the window of this is respect against offered and wader it\n",
      "every no more, without me, and he’llog being unagainst the ship?\n",
      "\n",
      "The bow’s own reipors rolling and long down, these\n",
      "things than him?” joining the deep, d must have\n",
      "burn thy liberoh. Es where goes aro; I feel\n",
      "sife and nearly used to which by\n",
      "to be chapted, never mankind of\n",
      "their full grown, lesson; holding up the three\n",
      "boat that he is subsobly invasine twoceaningly intolerably enough that however, that almost\n",
      "following this\n",
      "harpooneer like So May; the great Sperm Whale is at\n",
      "the wept hand about in one to a good plunging in the heedilled eyes atthe\n",
      "landsman for form and funck. There and then you belonged to a sweeping and den\n",
      "time, finly ranged them; furlongly roon; it was three to any vessel, the\n",
      "Cape of Tower, for exam, now ead to any sin; I will they\n",
      "are well entered by the conction in hands, studies no angels. And as to the\n",
      "seasfranken the foot-pieces. I knowl that he is pliciously dentire cunes recoged; and therewore\n",
      "were struck\n",
      "me coffleme; now merely\n",
      "become one word, with in their sunk deplicates and soder disastress approaced all you\n",
      "below’s deat it is obtained from about a little special in\n",
      "chains, the Sperm Whale prettures and stepping fifty, Ahab was excied, about it is a boarding, state-rig,\n",
      "though all this Laker, took the boat’s crew did I may in a wonder in their turns it about\n",
      "simultaneous against it to a sort of helmetain a leastly sufficiently for\n",
      "Queequeg’s s deverse! Loy? By Ahab’s land drink out\n",
      "most other! I move you so that.”\n",
      "\n",
      "Any whale, there will uses forth the boat with badinessed arms. And with wholing with his\n",
      "old voloured general.\n",
      "\n",
      "There seemed to go as if the reader of many houses alway, and the noblent lamp-beheard alone\n",
      "and then turned out intervals, however living sails, like\n",
      "bilyard-sea-head of scells, and as puous, to swo all this comparative and the\n",
      "entire chance is to her—keep your hearts of\n",
      "time, I filled on that her, why to block tight God; is one haunted boat entervation with\n",
      "your philoson ignocentanions to his feet! the whale-pole,\n",
      "but we murmured: “Why, d,” and,\n",
      "lance it! Pip!’ seeting him start. We thought senth its other and terminating which the sun, Bolievy him?”\n",
      "\n",
      "“Aye, aye! he’s a real relivius on the sperm whale’s hands on what a day, requising oar mounted so, is not the\n",
      "sometimes were there; tells there a senual lise upon cultues of\n",
      "men on the line in the\n",
      "great weight invests on the give chasue in the imosm himself; or towards the ship.\n",
      "\n",
      "(_Nuty and the prow; for you—archy what a hour till and a flood and mariners.\n",
      "The yellow thing these things there no other moyaging that terrivire a\n",
      "distinct shock; thou great Indians on boss intenses; oars! for God’s half-should have been\n",
      "no completed boat, hath something much mortals almost brief to spoke.\n",
      "\n",
      "It is tabelty, I thought he might almost seem to double of life, with him swim for\n",
      "no magnetic humps.\n",
      "\n",
      "Delician smoke for a long cirilar. During the last.\n",
      "\n",
      "At the whale’s gliding haltening in their moither,\n",
      "whatever gave me to rib\n",
      "over with his own, Moby Dick aught you ever cruising with these\n",
      "species of the wharper to rust.\n",
      "\n",
      "“My beyond besides the careful and aft—that’s the matter with his oak and under indeed. Nor would\n",
      "them do you stand, sir, it is; but the\n",
      "like sinking look at once, and at the pillions of the fine brow, to adgran\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, start_string=u\"Great\", num_generate = 10000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
